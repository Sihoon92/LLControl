"""
밀도계 데이터 전처리 모듈
기존 extract_raw_data_v1.2.txt를 클래스 기반으로 재구성
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Optional
import os
import logging


class DensitometerPreprocessor:
    """밀도계 데이터 전처리 클래스"""
    
    def __init__(self, config, logger: logging.Logger = None):
        """
        Parameters:
        -----------
        config : PreprocessConfig
            전처리 설정 객체
        logger : logging.Logger
            로거 객체
        """
        self.config = config
        self.logger = logger or logging.getLogger('coating_preprocessor.densitometer')
        self.extracted_data = None
    
    def run(
        self,
        changes_file: str,
        raw_data_file: str
    ) -> pd.DataFrame:
        """
        밀도계 데이터 추출 실행
        
        Parameters:
        -----------
        changes_file : str
            APC 전처리 결과 파일 (3rd_meaningful_changes.xlsx)
        raw_data_file : str
            밀도계 raw data 파일
        
        Returns:
        --------
        pd.DataFrame
            추출된 밀도계 데이터
        """
        self.logger.info("="*80)
        self.logger.info("밀도계 데이터 추출 시작")
        self.logger.info("="*80)
        self.logger.info(f"Changes 파일: {changes_file}")
        self.logger.info(f"Raw data 파일: {raw_data_file}")
        self.logger.info("="*80)
        
        self.extracted_data = self.extract_densitometer_data(
            changes_file=changes_file,
            raw_data_file=raw_data_file,
            output_file=os.path.join(
                self.config.OUTPUT_DIR,
                self.config.OUTPUT_DENSITOMETER
            ),
            before_minutes=self.config.BEFORE_MINUTES,
            after_minutes=self.config.AFTER_MINUTES
        )
        
        return self.extracted_data
    
    def extract_densitometer_data(
        self,
        changes_file: str,
        raw_data_file: str,
        output_file: str,
        before_minutes: int = 5,
        after_minutes: int = 5
    ) -> Optional[pd.DataFrame]:
        """
        APC 제어 변경 구간에 해당하는 밀도계 데이터 추출
        
        Parameters:
        -----------
        changes_file : str
            APC 전처리 결과 파일 (group_id, start_time, end_time 포함)
        raw_data_file : str
            밀도계 raw data 파일 (첫 칼럼: time, 나머지: 밀도계 L/L 값)
        output_file : str
            추출된 데이터를 저장할 파일명
        before_minutes : int
            start_time 기준 이전 몇 분까지 추출할지
        after_minutes : int
            end_time 기준 이후 몇 분까지 추출할지
        
        Returns:
        --------
        pd.DataFrame
            추출된 밀도계 데이터
        """
        # 1. meaningful changes 파일 로드
        self.logger.info(f"[1단계] '{changes_file}' 파일 로드 중...")
        try:
            changes_df = pd.read_excel(changes_file)
            self.logger.info(f"   ✓ {len(changes_df)} 개의 group 정보 로드 완료")
            self.logger.debug(f"   칼럼: {list(changes_df.columns)}")
        except FileNotFoundError:
            self.logger.error(f"   ✗ '{changes_file}' 파일을 찾을 수 없습니다.")
            return None
        except Exception as e:
            self.logger.error(f"   ✗ 오류: {e}", exc_info=True)
            return None
        
        # 필수 칼럼 확인
        required_cols = ['group_id', 'start_time', 'end_time']
        missing_cols = [col for col in required_cols if col not in changes_df.columns]
        if missing_cols:
            self.logger.error(f"   ✗ 필수 칼럼이 없습니다: {missing_cols}")
            return None
        
        # 2. 밀도계 raw data 로드
        self.logger.info(f"[2단계] '{raw_data_file}' 밀도계 raw data 로드 중...")
        try:
            # CSV 또는 Excel 파일 형식에 따라 로드
            if raw_data_file.endswith('.csv'):
                raw_df = pd.read_csv(raw_data_file)
            elif raw_data_file.endswith(('.xlsx', '.xls')):
                raw_df = pd.read_excel(raw_data_file)
            else:
                self.logger.error("   ✗ 지원하지 않는 파일 형식입니다.")
                return None
                
            self.logger.info(f"   ✓ {len(raw_df)} 행, {len(raw_df.columns)} 칼럼 로드 완료")
            self.logger.debug(f"   첫 번째 칼럼 (time): {raw_df.columns[0]}")
            
        except FileNotFoundError:
            self.logger.error(f"   ✗ '{raw_data_file}' 파일을 찾을 수 없습니다.")
            return None
        except Exception as e:
            self.logger.error(f"   ✗ 오류: {e}", exc_info=True)
            return None
        
        # 3. Value 칼럼 추출 (time 제외)
        self.logger.info(f"[3단계] Value 칼럼 식별 중...")
        time_col = raw_df.columns[0]
        
        # Value 칼럼 찾기
        value_columns = []
        for col in raw_df.columns[1:]:  # 첫 칼럼(time)은 제외
            col_str = str(col)
            # 'Value'로 시작하거나 숫자인 칼럼
            if 'value' in col_str.lower() or col_str.isdigit():
                value_columns.append(col)
        
        # Value 칼럼을 못 찾았으면 time 제외한 모든 칼럼
        if not value_columns:
            value_columns = [col for col in raw_df.columns if col != time_col]
        
        self.logger.info(f"   ✓ {len(value_columns)}개 Value 칼럼 발견")
        self.logger.debug(f"   첫 칼럼: {value_columns[0]}, 마지막 칼럼: {value_columns[-1]}")
        
        # 4. 중복 행 제거
        self.logger.info(f"[4단계] 중복 행 제거 중...")
        raw_df = self.remove_duplicate_rows(raw_df, value_columns)
        
        # 5. 시간 칼럼 파싱
        self.logger.info(f"[5단계] 시간 데이터 파싱 중...")
        
        # raw data의 시간을 datetime으로 변환
        raw_df['datetime'] = raw_df[time_col].apply(self._parse_time)
        self.logger.info(f"   ✓ 시간 데이터 변환 완료")
        self.logger.info(f"   시간 범위: {raw_df['datetime'].min()} ~ {raw_df['datetime'].max()}")
        
        # 6. 각 group에 대해 데이터 추출
        self.logger.info(f"[6단계] Group별 데이터 추출 중...")
        self.logger.info(f"   - start_time 기준 {before_minutes}분 전부터 추출")
        self.logger.info(f"   - end_time 기준 {after_minutes}분 후까지 추출")
        
        extracted_data_list = []
        
        for idx, row in changes_df.iterrows():
            group_id = row['group_id']
            start_time = self._parse_time(str(row['start_time']))
            end_time = self._parse_time(str(row['end_time']))
            
            # 추출 시간 범위 계산
            extract_start = start_time - timedelta(minutes=before_minutes)
            extract_end = end_time + timedelta(minutes=after_minutes)
            
            # 해당 시간 범위의 데이터 필터링
            mask = (raw_df['datetime'] >= extract_start) & (raw_df['datetime'] <= extract_end)
            group_data = raw_df[mask].copy()
            
            if len(group_data) == 0:
                self.logger.warning(f"   ⚠ Group {group_id}: 데이터 없음 (시간 범위: {extract_start} ~ {extract_end})")
                continue
            
            # before/after 구분
            # start_time 이전은 "before", start_time 이후는 "after"
            group_data['before/after'] = group_data['datetime'].apply(
                lambda x: 'before' if x < start_time else 'after'
            )
            
            # group_id 추가
            group_data['group_id'] = group_id
            
            # datetime 칼럼 제거 (원본 time 칼럼 유지)
            group_data = group_data.drop('datetime', axis=1)
            
            # 칼럼 순서 재정렬: [group_id, before/after, time, ...밀도계 칼럼들...]
            cols = ['group_id', 'before/after'] + [col for col in group_data.columns 
                                                     if col not in ['group_id', 'before/after']]
            group_data = group_data[cols]
            
            extracted_data_list.append(group_data)
            
            before_count = (group_data['before/after'] == 'before').sum()
            after_count = (group_data['before/after'] == 'after').sum()
            self.logger.info(f"   ✓ Group {group_id}: {len(group_data)} 행 추출 (before: {before_count}, after: {after_count})")
        
        # 7. 모든 데이터 합치기
        if not extracted_data_list:
            self.logger.warning("   ✗ 추출된 데이터가 없습니다.")
            return None
        
        self.logger.info(f"[7단계] 데이터 통합 중...")
        final_df = pd.concat(extracted_data_list, ignore_index=True)
        self.logger.info(f"   ✓ 총 {len(final_df)} 행 데이터 통합 완료")
        
        # 8. 결과 저장
        self.logger.info(f"[8단계] 결과 저장 중...")
        try:
            if output_file.endswith('.csv'):
                final_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            elif output_file.endswith(('.xlsx', '.xls')):
                final_df.to_excel(output_file, index=False)
            else:
                # 기본값: CSV로 저장
                output_file = output_file + '.csv'
                final_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            self.logger.info(f"   ✓ '{output_file}' 파일로 저장 완료")
            
            # 통계 정보 출력
            self.logger.info("="*80)
            self.logger.info("추출 완료 통계")
            self.logger.info("="*80)
            self.logger.info(f"총 Group 수: {final_df['group_id'].nunique()}")
            self.logger.info(f"총 데이터 행 수: {len(final_df)}")
            self.logger.info(f"Before 데이터: {(final_df['before/after'] == 'before').sum()} 행")
            self.logger.info(f"After 데이터: {(final_df['before/after'] == 'after').sum()} 행")
            self.logger.info(f"총 칼럼 수: {len(final_df.columns)}")
            self.logger.info("="*80)
            
        except Exception as e:
            self.logger.error(f"   ✗ 저장 오류: {e}", exc_info=True)
            return None
        
        return final_df
    
    def remove_duplicate_rows(
        self,
        df: pd.DataFrame,
        value_columns: List[str]
    ) -> pd.DataFrame:
        """
        연속되는 timestamp에서 모든 value 칼럼이 중복되는 행 제거
        (첫 번째 행은 유지하고 이후 연속 중복 행들만 삭제)
        
        Parameters:
        -----------
        df : pd.DataFrame
            밀도계 raw data
        value_columns : list
            밀도계 value 칼럼 리스트
        
        Returns:
        --------
        pd.DataFrame
            중복 제거된 데이터프레임
        """
        self.logger.info("="*80)
        self.logger.info("연속 중복 행 제거 시작")
        self.logger.info("="*80)
        self.logger.info(f"원본 데이터 행 수: {len(df)}")
        
        # 첫 번째 행은 항상 유지
        rows_to_keep = [0]
        
        # 이전 행과 현재 행의 value 칼럼들을 비교
        for idx in range(1, len(df)):
            current_values = df.iloc[idx][value_columns].values
            previous_values = df.iloc[idx-1][value_columns].values
            
            # 모든 value가 동일한지 확인
            # NaN 처리: NaN끼리는 동일하다고 판단
            is_duplicate = True
            for curr, prev in zip(current_values, previous_values):
                # 둘 다 NaN이면 동일
                if pd.isna(curr) and pd.isna(prev):
                    continue
                # 하나만 NaN이거나 값이 다르면 다른 행
                if pd.isna(curr) != pd.isna(prev) or curr != prev:
                    is_duplicate = False
                    break
            
            # 중복이 아니면 유지
            if not is_duplicate:
                rows_to_keep.append(idx)
        
        # 중복 제거된 데이터프레임 생성
        df_cleaned = df.iloc[rows_to_keep].reset_index(drop=True)
        
        removed_count = len(df) - len(df_cleaned)
        self.logger.info(f"제거된 중복 행 수: {removed_count} ({removed_count/len(df)*100:.2f}%)")
        self.logger.info(f"최종 데이터 행 수: {len(df_cleaned)}")
        self.logger.info("="*80)
        
        return df_cleaned
    
    def _parse_time(self, time_str, reference_date='2024-01-01'):
        """
        HH:MM:SS 형식의 시간 문자열을 datetime 객체로 변환
        
        Parameters:
        -----------
        time_str : str
            시간 문자열 (HH:MM:SS)
        reference_date : str
            기준 날짜 (시간만 있을 경우 날짜를 추가하기 위함)
        
        Returns:
        --------
        datetime
        """
        if isinstance(time_str, str):
            return pd.to_datetime(f"{reference_date} {time_str}")
        return time_str
