# MBRL (Model-Based Reinforcement Learning) ëª¨ë“ˆ

Per-Zone PETS (Probabilistic Ensembles with Trajectory Sampling) êµ¬í˜„

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
mbrl/
â”œâ”€â”€ __init__.py           # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”œâ”€â”€ config.py             # PETS ì„¤ì •
â”œâ”€â”€ ensemble_nn.py        # Per-Zone Neural Network
â”œâ”€â”€ dynamics_model.py     # í™•ë¥ ì  Dynamics Model
â”œâ”€â”€ data_processor.py     # ë°ì´í„° ì „ì²˜ë¦¬
â””â”€â”€ README.md            # ì´ íŒŒì¼
```

## ğŸ¯ ì„¤ê³„ ì² í•™

### CatBoostì™€ ë™ì¼í•œ êµ¬ì¡° (ê³µì •í•œ ë¹„êµ)

**Per-Zone ëª¨ë¸ë§**:
- ê° Zoneì„ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡
- ì¸ì ‘ Zone ì •ë³´ëŠ” ì…ë ¥ì— í¬í•¨ (i-1, i, i+1)
- ìœ„ì¹˜ íŠ¹ì„±ìœ¼ë¡œ ë¬¼ë¦¬ì  íŠ¹ì„± ë°˜ì˜

**ì…ë ¥ êµ¬ì¡°** (11ê°œ):
```
[
  ìœ„ì¹˜ íŠ¹ì„± (4): distance, edge_distance, normalized_position, normalized_distance
  í˜„ì¬ CLR (3): CLR_1, CLR_2, CLR_3
  ì œì–´ ë³€í™” (4): delta_GV_{i-1}, delta_GV_i, delta_GV_{i+1}, delta_RPM
]
```

**ì¶œë ¥ êµ¬ì¡°** (3ê°œ):
```
diff_CLR = [diff_CLR_1, diff_CLR_2, diff_CLR_3]
```

## ğŸ”§ ì£¼ìš” ì°¨ì´ì : CatBoost vs PETS

| íŠ¹ì§• | CatBoost | PETS |
|------|----------|------|
| ì˜ˆì¸¡ ë°©ì‹ | ì  ì˜ˆì¸¡ (ë‹¨ì¼ ê°’) | í™•ë¥  ë¶„í¬ (mean + variance) |
| ë¶ˆí™•ì‹¤ì„± | âŒ ì—†ìŒ | âœ… Aleatoric + Epistemic |
| ëª¨ë¸ íƒ€ì… | Gradient Boosting | Ensemble Neural Network |
| í•™ìŠµ ë°©ì‹ | íŠ¸ë¦¬ ì•™ìƒë¸” | Backpropagation |
| ì¶œë ¥ | y | (Î¼, ÏƒÂ²) |

## ğŸš€ ì‚¬ìš©ë²•

### 1. í™˜ê²½ ì„¤ì •

```bash
# PyTorch ì„¤ì¹˜ (í•„ìˆ˜)
pip install torch torchvision

# ê¸°íƒ€ ì˜ì¡´ì„±
pip install numpy pandas scikit-learn joblib
```

### 2. ë°ì´í„° ì¤€ë¹„

```python
from apc_optimization.mbrl import PETSDataProcessor

# ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±
processor = PETSDataProcessor(
    normalize=True,
    validation_split=0.2,
    test_split=0.1,
    random_seed=42
)

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
data_dict = processor.process(
    data_file='outputs/model_training_data.xlsx',
    mode='training'
)

# ê²°ê³¼: {'train': (X, y), 'val': (X, y), 'test': (X, y)}
X_train, y_train = data_dict['train']
X_val, y_val = data_dict['val']
X_test, y_test = data_dict['test']
```

### 3. ëª¨ë¸ í•™ìŠµ

```python
from apc_optimization.mbrl import PerZoneProbabilisticEnsemble
import torch

# ëª¨ë¸ ìƒì„±
model = PerZoneProbabilisticEnsemble(
    n_ensembles=5,
    input_dim=11,
    output_dim=3,
    hidden_dims=[128, 128],
    device='cpu'  # or 'cuda' if available
)

# Optimizer ì´ˆê¸°í™”
model.init_optimizers(lr=1e-3, weight_decay=1e-5)

# í•™ìŠµ ë£¨í”„
n_epochs = 100
batch_size = 256

for epoch in range(n_epochs):
    # ë°°ì¹˜ ìƒ˜í”Œë§
    indices = np.random.choice(len(X_train), batch_size, replace=False)
    X_batch = X_train[indices]
    y_batch = y_train[indices]

    # í•™ìŠµ step
    metrics = model.train_on_batch(X_batch, y_batch)

    if (epoch + 1) % 10 == 0:
        # ê²€ì¦
        val_metrics = model.evaluate_on_batch(X_val, y_val)
        print(f"Epoch {epoch+1}: Train Loss={metrics['loss']:.6f}, Val MSE={val_metrics['mse']:.6f}")

# ëª¨ë¸ ì €ì¥
model.save('outputs/models_v2/mbrl/pets_per_zone_best.pt')
```

### 4. ì˜ˆì¸¡ (ì „ì²´ 11ê°œ Zone)

```python
import numpy as np

# í˜„ì¬ ìƒíƒœ
current_clr_all = np.random.randn(11, 3)  # 11 zones Ã— 3 CLR
delta_gv = np.array([0.5, 0.3, ..., 0.1])  # 11ê°œ GV ë³€í™”
delta_rpm = 10.0

# ì˜ˆì¸¡
result = model.predict_all_zones(
    current_clr_all,
    delta_gv,
    delta_rpm,
    return_uncertainty=True
)

# ê²°ê³¼
diff_clr_mean = result['diff_clr_mean']          # (11, 3) - ì˜ˆì¸¡ í‰ê· 
diff_clr_uncertainty = result['diff_clr_uncertainty']  # (11, 3) - ë¶ˆí™•ì‹¤ì„±
next_clr = result['next_clr']                     # (11, 3) - ë‹¤ìŒ ìƒíƒœ

# ë¶ˆí™•ì‹¤ì„± í™œìš©
high_uncertainty_zones = np.where(diff_clr_uncertainty.mean(axis=1) > threshold)[0]
print(f"High uncertainty zones: {high_uncertainty_zones + 1}")  # Zone ID (1-based)
```

### 5. CatBoostì™€ ë¹„êµ

```python
from apc_optimization import CatBoostModelManager

# CatBoost ëª¨ë¸ ë¡œë“œ
catboost_model = CatBoostModelManager()

# PETS ëª¨ë¸ ë¡œë“œ
pets_model = PerZoneProbabilisticEnsemble(...)
pets_model.load('outputs/models_v2/mbrl/pets_per_zone_best.pt')

# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
for model_name, model in [('CatBoost', catboost_model), ('PETS', pets_model)]:
    if model_name == 'CatBoost':
        # CatBoost ì˜ˆì¸¡ (êµ¬í˜„ í•„ìš”)
        pass
    else:
        metrics = model.evaluate_on_batch(X_test, y_test)
        print(f"{model_name}: MSE={metrics['mse']:.6f}, MAE={metrics['mae']:.6f}")
        print(f"  RÂ²={metrics['r2']:.4f}, Uncertainty={metrics['mean_uncertainty']:.6f}")
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì§€í‘œ

### ì˜ˆì¸¡ ì •í™•ë„
- **MSE** (Mean Squared Error): ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ
- **MAE** (Mean Absolute Error): ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ
- **RÂ² Score**: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ

### ë¶ˆí™•ì‹¤ì„± (PETSë§Œ)
- **Calibration (ECE)**: ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±ì´ ì‹¤ì œ ì˜¤ì°¨ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
- **NLL** (Negative Log-Likelihood): ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ

### Open-loop ì‹œë®¬ë ˆì´ì…˜
- ì‹¤ì œ ê¶¤ì ì„ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì¬í˜„í•˜ëŠ”ê°€?

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

`config.py`ì—ì„œ ì„¤ì • ê°€ëŠ¥:

```python
# ëª¨ë¸ êµ¬ì¡°
DYNAMICS_MODEL_CONFIG = {
    'hidden_dims': [128, 128],  # Hidden layer í¬ê¸°
    'activation': 'relu',
    'use_layer_norm': True,
    'dropout': 0.0,
}

# ì•™ìƒë¸”
ENSEMBLE_CONFIG = {
    'n_ensembles': 5,  # 5~7 ê¶Œì¥
}

# í•™ìŠµ
TRAINING_CONFIG = {
    'batch_size': 256,
    'epochs': 100,
    'learning_rate': 1e-3,
    'weight_decay': 1e-5,
}
```

## ğŸ”¬ ì‹¤í—˜ ê¶Œì¥ì‚¬í•­

### Phase 1: Dynamics Model ë‹¨ë… í‰ê°€
1. PETS í•™ìŠµ
2. CatBoostì™€ ì˜ˆì¸¡ ì •í™•ë„ ë¹„êµ
3. ë¶ˆí™•ì‹¤ì„± êµì • ê²€ì¦

**ëª©í‘œ**: PETS MSE â‰¤ CatBoost MSE

### Phase 2: ì œì–´ê¸° í†µí•© (ë¯¸ë˜)
1. Planner (CEM) êµ¬í˜„
2. CatBoost + DE vs PETS + CEM ë¹„êµ

## ğŸ“ TODO

- [ ] Planner (CEM) êµ¬í˜„
- [ ] Trajectory Sampler êµ¬í˜„
- [ ] ëª¨ë¸ ë¹„êµ í”„ë ˆì„ì›Œí¬ êµ¬í˜„
- [ ] ì‹œê°í™” ë„êµ¬ ì¶”ê°€
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- PETS: [Chua et al., 2018](https://arxiv.org/abs/1805.12114)
- CatBoost: [Prokhorenkova et al., 2018](https://arxiv.org/abs/1706.09516)

---

**Author**: LLControl Team
**Version**: 0.1.0
**Last Updated**: 2024-02-13
